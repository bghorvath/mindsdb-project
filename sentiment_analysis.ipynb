{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# I. Training sentiment analysis model on Imdb reviews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1, Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/bghorvath/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## 2, Getting the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "save_filename = \"aclImdb_v1.tar.gz\"\n",
    "if not os.path.exists(save_filename):\n",
    "    req.urlretrieve(imdb_url, save_filename)\n",
    "\n",
    "imdb_folder = \"aclImdb\"\n",
    "if not os.path.exists(imdb_folder):\n",
    "    with tarfile.open(save_filename) as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('IMDb_Reviews.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "source": [
    "## 3, Read data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df = pd.read_csv('IMDb_Reviews.csv')\n",
    "\n",
    "y = df.sentiment.values\n",
    "X = df.review.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, \n",
    "                                                    shuffle=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "source": [
    "## 4, Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    tokens = [ps.stem(word) for word in text.split()]\n",
    "    return [w for w in tokens if not w in sw]\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False, \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_porter, \n",
    "                        use_idf=True, norm='l2',\n",
    "                        smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(cv=9,\n",
    "                          scoring='accuracy',\n",
    "                          random_state=0,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=3,\n",
    "                          max_iter=300)"
   ]
  },
  {
   "source": [
    "clf = make_pipeline(tfidf, logreg)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "saved_model = open('clf_model1.sav','wb')\n",
    "pickle.dump(clf, saved_model)\n",
    "saved_model.close()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5, Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'clf_model1.sav'\n",
    "clf = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9002"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "source": [
    "# II. Inference on tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1, Packages (again)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/bghorvath/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "import nest_asyncio\n",
    "import twint\n",
    "import csv\n",
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "source": [
    "## 1, Imdb dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-11-3521586ac01d>:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n  movie_data['usa_gross_income'] = movie_data['usa_gross_income'].str.replace('$','').replace('NaN',np.nan).astype(float)\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "movie_data = pd.read_csv('processed_IMDb_movies.csv')\n",
    "\n",
    "movie_data['usa_gross_income'] = movie_data['usa_gross_income'].str.replace('$','').replace('NaN',np.nan).astype(float)\n",
    "\n",
    "filtered_movies = movie_data[(movie_data['year']==2018) & (movie_data['country']=='USA') & (movie_data['usa_gross_income'] > 1000000)].loc[:][['imdb_title_id','original_title','year','date_published','usa_gross_income']].reset_index()\n",
    "\n",
    "del filtered_movies['index']"
   ]
  },
  {
   "source": [
    "## 2, Fetch tweets about selected movies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_days(iso_date,days):\n",
    "\n",
    "    d = date.fromisoformat(iso_date)\n",
    "    d_plus_days = d + timedelta(days=days)\n",
    "    \n",
    "    return d_plus_days.isoformat()\n",
    "\n",
    "def run_twint(dataframe):\n",
    "    #for i in range(len(dataframe)):\n",
    "    for i in range(3):\n",
    "        c = twint.Config()\n",
    "        c.Search = dataframe['original_title'][i]\n",
    "        c.Lang = 'en'\n",
    "        c.Limit = 100\n",
    "        c.Output = 'tweets/{}.csv'.format(i)\n",
    "        c.Email = False\n",
    "        c.Custom['tweet'] = ['tweet']\n",
    "        c.Phone = False\n",
    "        c.Links = 'exclude'\n",
    "        c.Pandas_clean = True\n",
    "        c.Filter_retweets = True\n",
    "        c.Hide_output = True\n",
    "        c.Store_csv = True\n",
    "        c.Since = dataframe['date_published'][i]\n",
    "        c.Until = add_days(dataframe['date_published'][i],3)\n",
    "        twint.run.Search(c)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[!] No more data! Scraping will stop now.\nfound 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "run_twint(filtered_movies)"
   ]
  },
  {
   "source": [
    "## 3, Sentiment analysis on tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    tokens = [ps.stem(word) for word in text.split()]\n",
    "    return [w for w in tokens if not w in sw]\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False, \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_porter, \n",
    "                        use_idf=True, norm='l2',\n",
    "                        smooth_idf=True)\n",
    "\n",
    "logreg = LogisticRegressionCV(cv=9,\n",
    "                          scoring='accuracy',\n",
    "                          random_state=0,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=3,\n",
    "                          max_iter=300)\n",
    "\n",
    "filename = 'clf_model1.sav'\n",
    "clf = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "sentiment_dataset = pd.DataFrame({'sentiment':[]}, columns = ['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tweets/0.csv\n",
      "tweets/5.csv\n",
      "tweets/1.csv\n",
      "tweets/2.csv\n"
     ]
    }
   ],
   "source": [
    "directory = 'tweets/'\n",
    "for filename in os.listdir(directory):\n",
    "    fn = os.path.join(directory, filename)\n",
    "    print(fn)\n",
    "    p = Path(fn)\n",
    "    with open(fn, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        a = []\n",
    "        for row in reader:\n",
    "            a.append(int(clf.predict(row)))\n",
    "        sentiment_dataset.loc[p.stem] = [statistics.mode(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentiment\n",
       "0        0.0\n",
       "5        0.0\n",
       "1        1.0\n",
       "2        0.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sentiment_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}